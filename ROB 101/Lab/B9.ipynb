{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3f4faaa",
   "metadata": {},
   "source": [
    "### IllumiDesk Tips\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c23b2eb",
   "metadata": {},
   "source": [
    "* Within code cells, click the blue play button on the left to run the code.\n",
    "* In the upper right, you can see if your kernel is connected. That is a dropdown menu that has additional functionality to “disconnect”, “restart”, and “clear all outputs” of the Kernel. None of these buttons will delete your work.\n",
    "* If you would like to test code, then you can access the console at the bottom of the page and play with code there.\n",
    "* Do not forget that code runs sequentially!\n",
    "* Selecting the clipboard icon on the right will allow you to skip to different parts of this document without scrolling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6a1ee0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126cd449",
   "metadata": {},
   "source": [
    "### Introduction to While Loops\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f9839f",
   "metadata": {},
   "source": [
    "Julia Documentation: [Control Flow · The Julia Language](https://docs.julialang.org/en/v1/manual/control-flow/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7413b7df",
   "metadata": {},
   "source": [
    "The first thing that happens when a `while` loop is executed is that the controlling Boolean expression is evaluated. If the Boolean expression evaluates to `false`, then the body of the loop is never executed. It might seem pointless to execute the body of a loop zero times, but that is sometimes the desired action.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089c6d68",
   "metadata": {},
   "source": [
    "For example, a `while` loop is often used to sum a list of numbers, but the list could be empty. To be more specific, a checkbook balancing program might use a `while` loop to sum the values of all checks you have written in a month, but you might take a month’s vacation and write no checks at all. In that case, there are zero numbers to sum, so the loop is iterated zero times.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07af6512",
   "metadata": {},
   "source": [
    "The body of a `while` loop should change the value of one or more global variables so that the condition becomes `false` eventually and the loop terminates. Otherwise, the loop will repeat forever, which is called an **infinite loop**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4db7247",
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `Boolean` not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `Boolean` not defined\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ d:\\CODING\\Julia\\Lab\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X11sZmlsZQ==.jl:3"
     ]
    }
   ],
   "source": [
    "#General Format of a While Loop\n",
    "\n",
    "while Boolean\\_Expression\\_Here\n",
    " #body of the loop\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8062caef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sumFor = 6\n",
      "sumWhile = 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using LinearAlgebra\n",
    "\n",
    "# Example: Sum of the entries in v\n",
    "\n",
    "v = [1;2;3]\n",
    "\n",
    "#=\n",
    "Contrary to a for loop, the while loop conditions can not be initalized \n",
    "inside of the loop header. The variables must be declared outside.\n",
    "=#\n",
    "\n",
    "# Notice i is declared in the header\n",
    "sumFor = 0\n",
    "  for i in 1:length(v)\n",
    "  sumFor += v[i]\n",
    "end\n",
    "@show sumFor\n",
    "\n",
    "\n",
    "#=\n",
    "Notice j is declared outside of the header\n",
    "\n",
    "Notice that inside of the loop, we have to increase j.\n",
    "\n",
    "- j will not increase itself in the same way that i does in the\n",
    "for loop above.\n",
    "\n",
    "- if I do not increase j, then the while loop will run forever\n",
    "=#\n",
    "sumWhile = 0\n",
    "j = 1\n",
    "while j <= length(v)\n",
    "  sumWhile += v[j]\n",
    "  j += 1 \n",
    "end\n",
    "\n",
    "#=\n",
    "If this @show statement is not here\n",
    "then we can not expect to see any output\n",
    "of any variables updated within the loop\n",
    "=#\n",
    "@show sumWhile "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3061069d",
   "metadata": {},
   "source": [
    "A `while` loop is used as opposed to a `for` loop when we are unsure of the number of iterations required to complete the task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db72c022",
   "metadata": {},
   "source": [
    "The Bisection Algorithm is a great example to show when a `for` loop could NOT be used, and a `while` loop is necessary. Although we may know the starting condition for the bisection algorithm, we do not know the ending condition which is required in a `for` loop. We don't know whether we need to loop 5 times or 100 times. However, we do know that we need to stop when we've found a root or when too many iterations have taken place. The latter prevents us from writing an infinite loop.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97973b3",
   "metadata": {},
   "source": [
    "### Break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f448de9",
   "metadata": {},
   "source": [
    "Sometimes, you don’t know it’s time to end a loop until you get halfway through the body. In that case, you can use the `break` statement to jump out of the loop.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7d64ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Break Example\n",
    "i = 0\n",
    "while true \n",
    "    i = i + 1\n",
    "    if i == 6\n",
    "        break\n",
    "    else\n",
    "        println(i) \n",
    "    end \n",
    "end "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f67c8d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d5c0f3",
   "metadata": {},
   "source": [
    "**Symbolic, Numerical, and Automatic Differentiation**\n",
    "======================================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d0ea80",
   "metadata": {},
   "source": [
    "**& the Glories of the Jacobian**\n",
    "=================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538278d1",
   "metadata": {},
   "source": [
    "**In this lab, we take a look at various ways of computing derivatives of mathematical functions**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94dbff1",
   "metadata": {},
   "source": [
    "**and make a robot dance using a Jacobian.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f29ff0c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Plots.GRBackend()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using Symbolics, LinearAlgebra, FiniteDiff, ForwardDiff, Plots\n",
    "gr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039dcc0c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28fa8a9",
   "metadata": {},
   "source": [
    "Symbolic Differentiation\n",
    "------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae8d81c",
   "metadata": {},
   "source": [
    "This is the kind of differentiation you learn (or will learn) in your intro Calculus classes. Using this approach, you can take derivatives of expressions using the various rules of differential calculus, such as:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c532e032",
   "metadata": {},
   "source": [
    "\\* \\frac{d}{d x} c = 0 *where* c *is a numeric constant*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce4783e",
   "metadata": {},
   "source": [
    "### \\*\\frac{d}{d x} x = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ec6d43",
   "metadata": {},
   "source": [
    "\\* \\frac{d}{d x} y = 0 *where* y *is a variable other* *than* x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e6823d",
   "metadata": {},
   "source": [
    "### \\*\\frac{d}{dx} (u + v) = \\frac{d}{d x} u + \\frac{d}{d x} v = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6ea638",
   "metadata": {},
   "source": [
    "### *\\**\\frac{d}{dx} (u \\cdot v) = u \\cdot \\frac{d}{d x} v + v \\cdot \\frac{d}{dx} u\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2554c76b",
   "metadata": {},
   "source": [
    "### *\\**\\frac{d}{d x} \\frac{u}{v} = \\frac{(v \\* \\frac{d}{d x}u - u \\* \\frac{d}{dlx} v)}{v^2}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f5866a",
   "metadata": {},
   "source": [
    "\\*…and so on\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa63edc",
   "metadata": {},
   "source": [
    "This form of differentiation is called symbolic, because it allows you to perform differentiation by manipulating symbols, or more precisely, variables, without actually evaluating them real numbers. This is a big part of Calc I.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1b3002",
   "metadata": {},
   "source": [
    "As you may have guessed, Julia has a sweet package called `Symbolics.jl` which allows you to take symbolic derivatives of input expressions. The next few cells will teach you how to use this package.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4b4fe6",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af7103c",
   "metadata": {},
   "source": [
    "Find the derivative of \\frac{df}{dx} of the function f(x) = x³ cos(X) + x² sin(x)² + log(3x) using symbolic differentiation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d155c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=\n",
    "@variables is a macro\n",
    "that declares the variables\n",
    "in the function\n",
    "=#\n",
    "@variables x\n",
    "\n",
    "#differential operator \n",
    "d╱dx = Differential(x)\n",
    "\n",
    "#define f(x) \n",
    "f = x^3*cos(x) + x^2*sin(x)^2 + log(3x)\n",
    "\n",
    "#apply the differential operator\n",
    "@show d╱dx(f)\n",
    "\n",
    "#= \n",
    "compute the partial derivative\n",
    "using the expand_derivatives function\n",
    "=#\n",
    "@show dfdx = expand_derivatives(d╱dx(f))\n",
    "\n",
    "#=\n",
    "evaluate the derivative expression at some value\n",
    "using the substitute function\n",
    "\n",
    "x = 0.5\n",
    "=#\n",
    "@show val = substitute(dfdx, (Dict(x=>0.5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2635d8e1",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a21b05d",
   "metadata": {},
   "source": [
    "Numerical Differentiation\n",
    "-------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee32a98",
   "metadata": {},
   "source": [
    "Numerical differentiation is another approach for taking derivatives of functions. Unlike Symbolic differentiation where we derive exact, analytical derivatives of functions, Numerical differentiation estimates the derivatives of functions using values of the function at different points.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a467e42",
   "metadata": {},
   "source": [
    "The simplest and most common method for Numerical Differentiation is **Finite Difference**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5ac658",
   "metadata": {},
   "source": [
    "Finite Difference approximates the derivative \\frac{\\partial f}{\\partial x} of a function f at some numerical point x\\_0 as \\frac{\\partial f(x\\_0)}{\\partial x} = \\frac{f(x\\_0+h) - f(x\\_0)}{h}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b8974f",
   "metadata": {},
   "source": [
    "This finite difference variety is specifically known as the Forward Difference approximation. There also exists a Backward Difference approximation which is expressed as \\frac{\\partial f(x\\_0)}{\\partial x} = \\frac{f(x\\_0) - f(x\\_0-h)}{h} and Symmetric Difference approximation expressed as \\frac{\\partial f(x\\_0)}{\\partial x} = \\frac{f(x\\_0+h) - f(x\\_0-h)}{2h}.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397a27c9",
   "metadata": {},
   "source": [
    "If the derivative off(x) exists at point x\\_0, for some small h, the forward, backward and symmetric difference approximations are approximately equal. If they end up being significantly different, then the function f(x) is not differentiable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071ddc75",
   "metadata": {},
   "source": [
    "As you would have guessed, there exists a really sweet Julia package with implementations of finite differences called `FiniteDiff.jl`. `FiniteDiff.jl` has empirically the fastest implementation of finite differences in any programming language!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfcd0c3",
   "metadata": {},
   "source": [
    "In the following cells, we are going to learn how to use it to take derivatives, gradients, Jacobians and Hessians of functions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d6ae71",
   "metadata": {},
   "source": [
    "The main difference between derivatives and gradients in relation to the `FiniteDiff.jl` package is that, derivatives are used for functions with scalar inputs and gradients are used for functions with vector inputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f98f58",
   "metadata": {},
   "source": [
    "### Example 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edcf087",
   "metadata": {},
   "source": [
    "Find the derivative of \\frac{df}{dx} of the function f(x) = x³ cos(X) + x² sin(x)² + log(3x) using finite differences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2629d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function\n",
    "g(x) = x^3*cos(x) + x^2*sin(x)^2 + log(3x)\n",
    "\n",
    "#=\n",
    "derivative of g at x = 0.5\n",
    "using the function \n",
    "FiniteDiff.finite_difference_derivative\n",
    "\n",
    "First argument is the function\n",
    "Second argument is the point of interest\n",
    "=#\n",
    "val = FiniteDiff.finite_difference_derivative(g, 0.5)\n",
    "\n",
    "# Solution almost equal to Symbolic from Ex. 1!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6912b62",
   "metadata": {},
   "source": [
    "### Example 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ea3f58",
   "metadata": {},
   "source": [
    "Compute the Jacobian of the function below using finite differences where x \\in \\mathbb{R}³ and x\\_i is the i-th element in x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890175c3",
   "metadata": {},
   "source": [
    "h(x) = \\begin{bmatrix} x\\_1^3cos(x\\_2) + x\\_2^2sin(x\\_1)^2 \\\\ log(3x\\_3) \\end{bmatrix}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209155d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the function\n",
    "h(x) = [x[1]^3*cos(x[2]) + x[2]^2*sin(x[1])^2;\n",
    "                        log(3x[3])]\n",
    "\n",
    "# the point of interest\n",
    "x₀ = [0.1, 0.5, 0.7]\n",
    "\n",
    "#Jacobian\n",
    "J = FiniteDiff.finite_difference_jacobian(h, x₀)\n",
    "\n",
    "#Gradients & Hessians can be computed similarly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fbb27b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f178df8",
   "metadata": {},
   "source": [
    "Automatic Differentiation\n",
    "-------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124223c0",
   "metadata": {},
   "source": [
    "Similar to Symbolic Differentiation, Automatic Differentiation uses differential calculus rules to compute derivatives. However, its goal is to return a numerical solution instead of a single closed-form analytical expression. As such, Automatic Differentiation keeps track of intermediate variables and their derivatives to speed up computation. It decomposes the function into primitive operations using the chain rule, evaluates the operations and their derivatives and stores them in intermediate variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b94f29",
   "metadata": {},
   "source": [
    "To appreciate how automatic differentiation works, let's take a look at an example.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c895db0",
   "metadata": {},
   "source": [
    "We would like to compute the Jacobian of the multivariate function:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930d7e76",
   "metadata": {},
   "source": [
    "f(x,y) = y\\cdot sin(x) + y^2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085d8c99",
   "metadata": {},
   "source": [
    "at a point x=0.1, y=0.3 using automatic differentiation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d67776",
   "metadata": {},
   "source": [
    "First, let us define the (^\\bullet)\\_x operator as the derivative with respect to x, \\frac{\\partial}{\\partial x}. The notation \\dot{a} for the derivative is due to Isaac Newton <https://en.wikipedia.org/wiki/Notation_for_differentiation>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099b6f70",
   "metadata": {},
   "source": [
    "Next, we will decompose f(x,y) into primitive operations and store their values and derivatives with respect to x in intermediate variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db4579f",
   "metadata": {},
   "source": [
    "(View **dx-5.png** in the attachments) This routine is called a **forward pass** with respect to x. It is worth noting that, unlike symbolic differentiation, the numerical values of each intermediate variable is computed in each row of the table.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341cb725",
   "metadata": {},
   "source": [
    "Similarly, we define the (^\\bullet)\\_y operator as the derivative with respect to y, \\frac{\\partial}{\\partial y} and define the intermediate variables and derivatives with respect to y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cff7c0",
   "metadata": {},
   "source": [
    "(View **dy.png** in the attachments)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934c5ca4",
   "metadata": {},
   "source": [
    "The Jacobian J of f(x,y) can now be defined as J = [\\dot{d}\\_x~~~\\dot{d}\\_y] = [0.2985 ~~~ 0.6998]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c941a0",
   "metadata": {},
   "source": [
    "The computational routine we have just demonstrated above is a special kind of Automatic Differentiation called Forward-Mode Automatic Differentiation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c301952f",
   "metadata": {},
   "source": [
    "In general, you would perform a forward pass for each argument to your function f. If the function argument is a vector, you would perform a forward pass for each element in the vector. The computational complexity is of the order O(n). You can imagine that, this could end up being computationally inefficient for functions f : \\mathbb{R}^n \\rightarrow \\mathbb{R}^m with n >>> m. In such cases, another special kind of Automatic Differentiation called Reverse-mode Automatic Differentiation will be much more appropriate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88f0af8",
   "metadata": {},
   "source": [
    "Reverse-mode Automatic Differentiation first performs a forward pass to compute **only** the intermediate variable values without taking the derivatives. It then performs a reverse (backwards) pass to compute the derivatives of the intermediate variables. A function f : \\mathbb{R}^n \\rightarrow \\mathbb{R}^m, will require m reverse passes to compute the Jacobian of the function. Because of this, Reverse-Mode automatic differentiation is suitable for functions where n >>> m and inefficient for functions were m >>> n.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97afb9f",
   "metadata": {},
   "source": [
    "As you may rightly guess, Julia has packages that have super-fast implementations of forward-mode and reverse-mode automatic differentiation named `ForwardDiff.jl` and `ReverseDiff.jl`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1141a81a",
   "metadata": {},
   "source": [
    "In the following cells, we will demonstrate the usage of `ForwardDiff.jl` for computing derivatives and Jacobians.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dfaac9",
   "metadata": {},
   "source": [
    "### Example 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b60ee6e",
   "metadata": {},
   "source": [
    "Find the derivative \\frac{d f\\_2}{d x} of the function f\\_2(x) = x^3cos(x) + x^2sin(x)^2 + log(3x) using Forward-mode Automatic Differentiation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f66cdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function\n",
    "f₂(x) = x^3*cos(x) + x^2*sin(x)^2 + log(3x)\n",
    "\n",
    "#=\n",
    "Evaluate derivative at x =0.5\n",
    "using the ForwardDiff.derivative function\n",
    "\n",
    "First argument is function\n",
    "Second argument is the pointof interest\n",
    "=#\n",
    "x = 0.5\n",
    "df₂╱dx = ForwardDiff.derivative(f₂, x)\n",
    "\n",
    "#Same result as Symbolic! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98edee6f",
   "metadata": {},
   "source": [
    "### Example 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f6659f",
   "metadata": {},
   "source": [
    "Compute the Jacobian of the function below using Forward-mode Automatic Differentiation where x \\in \\mathbb{R}^3 and x\\_i is the i-th element in x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87ba30c",
   "metadata": {},
   "source": [
    "h\\_2(x) = \\begin{bmatrix} x\\_1^3cos(x\\_2) + x\\_2^2sin(x\\_1)^2 \\\\ log(3x\\_3) \\end{bmatrix}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b505155d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the function\n",
    "h₂(x) = [x[1]^3*cos(x[2]) + x[2]^2*sin(x[1])^2;\n",
    "                        log(3x[3])]\n",
    "\n",
    "#point of interest\n",
    "x₀ = [0.1, 0.5, 0.7]\n",
    "\n",
    "#Jacobian\n",
    "J = ForwardDiff.jacobian(h₂, x₀)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062ebbe8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd879aca",
   "metadata": {},
   "source": [
    "### **Advantages & Disadvantages of Each Differentiation**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08ea20d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "| Differentiation | Advantages |\n",
    "| --- | --- |\n",
    "| Symbolic | * Symbolic Differentiation allows you to compute exact derivatives up to machine precision.\n",
    "* Symbolic Differentiation is also a great tool for analysis. Being able to generate closed-form derivatives of a model facilitates the understanding of the dynamics of the model.\n",
    "* Symbolic Differentiation is also significantly useful for generating closed-form derivatives of functions as code. Closed-loop controllers running on real-time systems often have to run at frequencies in the 1kHz range. This constrains the control loop to a time budget of 1 millisecond. Given this time budget, operations in the controller (such as computing derivatives and Jacobians) have to be as fast as possible. Being able to generate closed-form parameterized functions for derivatives and Jacobians offline using Symbolic differentiation that can then be evaluated online in microseconds can be very useful in speeding up your controller and meeting the controller time budget.\n",
    " |\n",
    "| Numerical | * Numerical differentiation is the simplest differentiation method to implement. It only depends on the function values and not on any set of differential calculus rules.\n",
    "* In view of this, numerical differentiation can be applied to any differentiable function to get reasonably accurate results.\n",
    "* \n",
    " |\n",
    "| Automatic | * Automatic Differentiation is very fast at computing derivatives. Because of this, automatic differentiation is the primary differentiation tool for computing derivatives when training deep neural networks.\n",
    "* Unlike with Finite Differences, Automatic Differentiation computes exact derivatives to machine precision.\n",
    "* Implementations of Automatic Differentiation in popular Deep Learning frameworks like Pytorch and Tensorflow build *Computational Graphs* when performing the forward pass. These computational graphs allow for easy debugging and analysis.\n",
    " |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fc600d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "| Differentiation | Disadvantages |\n",
    "| --- | --- |\n",
    "| Symbolic | The main disadvantage of symbolic derivatives is that, they tend to get gnarly and exponentially long very quickly. This phenomenon is called *Expression swell*. Imagine taking the derivative of a function h(x) = f(x)g(x) where f(x) is itself the product f(x) = u(x)v(x), the derivative of h(x) ends up as this gnarly function \\frac{d}{dx}h(x) = \\Bigg(\\frac{d}{dx}u(x)v(x) + u(x)\\frac{d}{dx}v(x)\\Bigg)g(x) + u(x)v(x)\\frac{d}{dx}g(x) leading to inefficient code. |\n",
    "| Numerical | * Finite Difference suffers from *Truncation error* resulting in inaccurate derivates when the step size is large.\n",
    "* Finite Difference can also be expensive to compute for functions with large input dimensions.\n",
    " |\n",
    "| Automatic | Automatic Differentiation is not as easy to implement as Finite Differences. Care usually has to be taken to implement it in a manner that is memory-efficient, particularly for reverse-mode automatic differentiation. Regardless, there are super-fast and clean implementations of automatic differentiation like [ForwardDiff.jl](https://juliadiff.org/ForwardDiff.jl/stable/) and [Zygote.jl](https://fluxml.ai/Zygote.jl/latest/) that exist off-the-shelf and can be applied to any use. |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6306da60",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8516764",
   "metadata": {},
   "source": [
    "**A bunch of helper code for the next section!**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2ed2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct Problem \n",
    "    tasks::Vector{Symbol} \n",
    "    sys\n",
    "end\n",
    "\n",
    "mutable struct Keypoint\n",
    "    head_pose::Union{Nothing, Vector}\n",
    "    lefthand_pose::Union{Nothing, Vector} \n",
    "    righthand_pose::Union{Nothing, Vector} \n",
    "    waist_pose::Union{Nothing, Vector}\n",
    "    frequency::Int\n",
    "end \n",
    "\n",
    "\n",
    "mutable struct PickleRick \n",
    "    θ::Vector{Float64}\n",
    "    θ̇ ::Vector{Float64} \n",
    "    body_observables\n",
    "    robot_keypoint_observables \n",
    "    obstacle_observables\n",
    "    tail_observable\n",
    "    dynamic::Bool \n",
    "    trail::Bool\n",
    "    show_contacts::Bool \n",
    "    show_com::Bool \n",
    "    show_support_polygon::Bool\n",
    "    show_goal::Bool\n",
    "    show_obstacle::Bool\n",
    "    l0::Float64\n",
    "    l1::Float64 \n",
    "    l2::Float64 \n",
    "    l3::Float64 \n",
    "    l4::Float64\n",
    "    l5::Float64 \n",
    "    l6::Float64\n",
    "    l7::Float64 \n",
    "    l8::Float64\n",
    "    l9::Float64 \n",
    "    l10::Float64\n",
    "    w::Float64\n",
    "    g::Union{Nothing, Vector{Vector{Float64}}}\n",
    "    task_maps::Vector{Symbol}\n",
    "    θᵣ::Vector{Float64}\n",
    "    Δt::Float64\n",
    "    m_links::Float64 \n",
    "    m_head::Float64\n",
    "    com_observable\n",
    "    priorities::Dict\n",
    "    max_range::Float64\n",
    "    keypoints::Vector{Keypoint}\n",
    "    kpindex::Int\n",
    "\n",
    "    function PickleRick(joint_positions, \n",
    "                        joint_velocities, goal_position, keypoints)\n",
    "        time_step = 10E-4\n",
    "        task_maps = [:posture, :repeller, :lbalance, :rbalance, :lefthand_attractor]\n",
    "        θᵣ = [π/2, 2π/3, π/6, 2π/3, π/6, π/2, π/3, 7π/12, 2π/3, 5π/12]\n",
    "        m_links = 0.05 #kg\n",
    "        m_head = 0.1 #kg\n",
    "        max_range = 10.0\n",
    "        priorities= Dict(:lefthand_attractor=>1,\n",
    "                         :posture=>2,\n",
    "                         :lbalance=>1,\n",
    "                         :rbalance=>1,\n",
    "                         :repeller=>2\n",
    "        )\n",
    "    \n",
    "\n",
    "        new(joint_positions, \n",
    "            joint_velocities, nothing, nothing, nothing, nothing, \n",
    "            false, false, false, false, false, false, true, 0.75, 1.0, \n",
    "            1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, goal_position, \n",
    "            task_maps, θᵣ, time_step, m_links, m_head, nothing, priorities,\n",
    "            max_range, keypoints, 1)\n",
    "    end\n",
    "end\n",
    "\n",
    "## Kinematics\n",
    "# foot as reference\n",
    "function link_poses(θ, sys::PickleRick)\n",
    "    l1 = sys.l1\n",
    "    l2 = sys.l2\n",
    "    l3 = sys.l3\n",
    "    l4 = sys.l4\n",
    "    l5 = sys.l5\n",
    "    l6 = sys.l6\n",
    "    l7 = sys.l7\n",
    "    l8 = sys.l8\n",
    "    l9 = sys.l9\n",
    "    l10 = sys.l10\n",
    "    l0 = sys.l0\n",
    "    w = sys.w \n",
    "    x8 = -w\n",
    "    y8 = 0.0\n",
    "    x10 = w \n",
    "    y10 = 0.0\n",
    "    x7 = x8 + l8 * cos(θ[8])\n",
    "    y7 = y8 + l8 * sin(θ[8])\n",
    "    x9 = x10 + l10 * cos(θ[10])\n",
    "    y9 = y10 + l10 * sin(θ[10]) \n",
    "    x6 = 0.5*(x7 + l7 * cos(θ[7]) + x9 + l9 * cos(θ[9]))\n",
    "    y6 = 0.5*(y7 + l7 * sin(θ[7]) + y9 + l9 * sin(θ[9]))\n",
    "    x1 = x6 + l1 * cos(θ[6])\n",
    "    y1 = y6 + l1 * sin(θ[6])\n",
    "    x2 = x1 - l2 * sin(θ[2])\n",
    "    y2 = y1 + l2 * cos(θ[2])\n",
    "    x3 = x2 - l3 * sin(θ[3])\n",
    "    y3 = y2 + l3 * cos(θ[3])\n",
    "    x4 = x1 + l4 * sin(θ[4])\n",
    "    y4 = y1 + l4 * cos(θ[4])\n",
    "    x5 = x4 + l5 * sin(θ[5])\n",
    "    y5 = y4 + l5 * cos(θ[5])\n",
    "    x0 = x1 + l0 * cos(θ[1])\n",
    "    y0 = y1 + l0 * sin(θ[1])\n",
    "    xn = x1 + 0.5*l0 * cos(θ[1])\n",
    "    yn = y1 + 0.5*l0 * sin(θ[1]) \n",
    "\n",
    "    return [[[x6, y6], [x7, y7], [x8, y8]], [[x6, y6], [x9, y9], [x10, y10]], \n",
    "    [[x6, y6], [x1, y1], [x0, y0]], [[x1, y1], [x4, y4], [x5, y5]], [[x1, y1], [x2, y2], [x3, y3]], [[xn, yn]]]\n",
    "end\n",
    "\n",
    "function left_hand_pose(θ, sys::PickleRick)\n",
    "    poses = link_poses(θ, sys::PickleRick)\n",
    "    return poses[5][end]\n",
    "end\n",
    "\n",
    "function right_hand_pose(θ, sys::PickleRick)\n",
    "    poses = link_poses(θ, sys::PickleRick)\n",
    "    return poses[4][end]\n",
    "end\n",
    "\n",
    "function left_foot_pose(θ, sys::PickleRick)\n",
    "    poses = link_poses(θ, sys::PickleRick)\n",
    "    return poses[1][end]\n",
    "end\n",
    "\n",
    "function right_foot_pose(θ, sys::PickleRick)\n",
    "    poses = link_poses(θ, sys::PickleRick)\n",
    "    return poses[2][end]\n",
    "end\n",
    "\n",
    "function head_pose(θ, sys::PickleRick)\n",
    "    poses = link_poses(θ, sys::PickleRick)\n",
    "    return poses[3][end]\n",
    "end\n",
    "\n",
    "function neck_pose(θ, sys::PickleRick)\n",
    "    poses = link_poses(θ, sys::PickleRick)\n",
    "    return poses[6][end]\n",
    "end\n",
    "\n",
    "function waist_pose(θ, sys::PickleRick)\n",
    "    poses = link_poses(θ, sys)\n",
    "    return poses[5][1]\n",
    "end\n",
    "\n",
    "function compute_COM(θ, env::PickleRick)\n",
    "    l1 = env.l1\n",
    "    l2 = env.l2\n",
    "    l3 = env.l3\n",
    "    l4 = env.l4\n",
    "    l5 = env.l5\n",
    "    l6 = env.l6\n",
    "    l7 = env.l7\n",
    "    l8 = env.l8\n",
    "    l9 = env.l9\n",
    "    l10 = env.l10\n",
    "    l0 = env.l0\n",
    "    w = env.w \n",
    "    x8 = -w\n",
    "    y8 = 0.0\n",
    "    x10 = w \n",
    "    y10 = 0.0\n",
    "    x7 = x8 + l8 * cos(θ[8])\n",
    "    y7 = y8 + l8 * sin(θ[8])\n",
    "    x9 = x10 + l10 * cos(θ[10])\n",
    "    y9 = y10 + l10 * sin(θ[10]) \n",
    "    x6 = 0.5*(x7 + l7 * cos(θ[7]) + x9 + l9 * cos(θ[9]))\n",
    "    y6 = 0.5*(y7 + l7 * sin(θ[7]) + y9 + l9 * sin(θ[9]))\n",
    "    x1 = x6 + l1 * cos(θ[6])\n",
    "    y1 = y6 + l1 * sin(θ[6])\n",
    "    x2 = x1 - l2 * sin(θ[2])\n",
    "    y2 = y1 + l2 * cos(θ[2])\n",
    "    x3 = x2 - l3 * sin(θ[3])\n",
    "    y3 = y2 + l3 * cos(θ[3])\n",
    "    x4 = x1 + l4 * sin(θ[4])\n",
    "    y4 = y1 + l4 * cos(θ[4])\n",
    "    x5 = x4 + l5 * sin(θ[5])\n",
    "    y5 = y4 + l5 * cos(θ[5])\n",
    "    x0 = x1 + l0 * cos(θ[1])\n",
    "    y0 = y1 + l0 * sin(θ[1])\n",
    "\n",
    "    xcm = (sum([env.m_links*x for x in [x1, x2, x3, x4, x5, x6, x7, x8, x9, x10]]) + env.m_head*x0) / (10*env.m_links + env.m_head)\n",
    "    ycm = (sum([env.m_links*y for y in [y1, y2, y3, y4, y5, y6, y7, y8, y9, y10]]) + env.m_head*y0) / (10*env.m_links + env.m_head)\n",
    "    \n",
    "    return (xcm, ycm)\n",
    "end\n",
    "\n",
    "\n",
    "## Simulation\n",
    "function step_plots!(θ̈ ::Vector{Float64}, prob::Problem, sys::PickleRick)\n",
    "    θ = sys.θ + θ̈ *sys.Δt\n",
    "    θ̇ = θ̈   \n",
    "    vellims = 10.0*ones(length(θ))\n",
    "    θ̇  = clamp.(θ̇ , -vellims, vellims)\n",
    "\n",
    "    sys.θ = θ\n",
    "    sys.θ̇ = θ̇ \n",
    "end\n",
    "\n",
    "\n",
    "## Visualization\n",
    "function visualize_plots!(sys::PickleRick)\n",
    "\n",
    "    θ = sys.θ \n",
    "    chains = link_poses(θ, sys)\n",
    "    chain1 = [[a[1], a[2]] for a in chains[1]]\n",
    "    chain2 = [[a[1], a[2]] for a in chains[2]]\n",
    "    chain3 = [[a[1], a[2]] for a in chains[3]]\n",
    "    chain4 = [[a[1], a[2]] for a in chains[4]]\n",
    "    chain5 = [[a[1], a[2]] for a in chains[5]]\n",
    "    head =   [[chains[3][3][1], chains[3][3][2]]]\n",
    "    Plots.plot() \n",
    "    Plots.plot!([c[1] for c in chain1], [c[2] for c in chain1] ; \n",
    "        linewidth=5, color=:purple)\n",
    "    Plots.plot!([c[1] for c in chain2], [c[2] for c in chain2]; \n",
    "        linewidth=5, color=:purple)\n",
    "    Plots.plot!([c[1] for c in chain3], [c[2] for c in chain3]; \n",
    "        linewidth=5, color=:purple)\n",
    "    Plots.plot!([c[1] for c in chain4], [c[2] for c in chain4]; \n",
    "        linewidth=5, color=:purple)\n",
    "    Plots.plot!([c[1] for c in chain5], [c[2] for c in chain5]; \n",
    "        linewidth=5, color=:purple)\n",
    "\n",
    "    Plots.scatter!([c[1] for c in chain1], [c[2] for c in chain1], \n",
    "        color=:black, markersize=8)\n",
    "    Plots.scatter!([c[1] for c in chain2], [c[2] for c in chain2], \n",
    "        color=:black, markersize=8)\n",
    "    Plots.scatter!([c[1] for c in chain3], [c[2] for c in chain3], \n",
    "        color=:black, markersize=8)\n",
    "    Plots.scatter!([c[1] for c in chain4], [c[2] for c in chain4], \n",
    "        color=:black, markersize=8)\n",
    "    Plots.scatter!([c[1] for c in chain5], [c[2] for c in chain5], \n",
    "        color=:black, markersize=8)\n",
    "    fig = Plots.scatter!([c[1] for c in head], [c[2] for c in head], color=:black, \n",
    "        markersize=20, legend=false, grid=false, aspect_ratio=:equal, \n",
    "        ylims=(-Inf, 4),\n",
    "        border=:none) \n",
    "    return fig\n",
    "end\n",
    "\n",
    "## Control \n",
    "function posture_task_map(θ, prob::Problem)\n",
    "    sys = prob.sys\n",
    "    g = sys.θᵣ\n",
    "    return θ - g\n",
    "end\n",
    "\n",
    "function lefthand_attractor_task_map(θ, prob::Problem)\n",
    "    sys = prob.sys\n",
    "    x₉ = sys.keypoints[sys.kpindex].lefthand_pose\n",
    "    xh = left_hand_pose(θ, sys)\n",
    "    return xh - x₉\n",
    "end\n",
    "\n",
    "function righthand_attractor_task_map(θ, prob::Problem)\n",
    "    sys = prob.sys\n",
    "    x₉ = sys.keypoints[sys.kpindex].righthand_pose\n",
    "    xh = right_hand_pose(θ, sys)\n",
    "    return xh - x₉\n",
    "end\n",
    "\n",
    "function head_attractor_task_map(θ, prob::Problem)\n",
    "    sys = prob.sys\n",
    "    x₉ = sys.keypoints[sys.kpindex].head_pose\n",
    "    xh = head_pose(θ, sys)\n",
    "    return xh - x₉\n",
    "end\n",
    "\n",
    "function waist_attractor_task_map(θ, prob::Problem)\n",
    "    sys = prob.sys\n",
    "    x₉ = sys.keypoints[sys.kpindex].waist_pose \n",
    "    xh = head_pose(θ, sys)\n",
    "    return xh - x₉\n",
    "end\n",
    "\n",
    "function dodge_task_map(θ, prob::Problem)\n",
    "    sys = prob.sys\n",
    "    os = sys.obstacle_observables\n",
    "    rs = sys.r\n",
    "    chains = link_poses(θ, sys)\n",
    "    xs = Float64[]; o = os[1]; r = 2*rs[1]\n",
    "    lh = chains[5][end]; rh = chains[4][end]\n",
    "    lẍ= chains[1][end]; rẍ= chains[2][end]\n",
    "    hd = chains[3][end]; nk = chains[6][1]\n",
    "    kp = [lh, rh, hd, nk]#[hd, lh, rh, lf, rf]\n",
    "    for k in kp \n",
    "        Δ = (norm(k - o.val)/r)[1] - 1.0\n",
    "        push!(xs, Δ)\n",
    "    end \n",
    "    # @show xs\n",
    "    return xs\n",
    "end\n",
    "\n",
    "### Potentials \n",
    "function posture_potential(x, ẋ, prob::Problem)\n",
    "    sys = prob.sys\n",
    "    M = 1.6\n",
    "    K = M/3\n",
    "    ϕ(x) = 0.5*x'*K*x\n",
    "    δₓ = FiniteDiff.finite_difference_gradient(ϕ, x)\n",
    "    f = -K*δₓ\n",
    "    return f\n",
    "end\n",
    "\n",
    "function lefthand_attractor_potential(x, ẋ, prob::Problem)\n",
    "    sys = prob.sys\n",
    "    M = 1.6\n",
    "    K = 2*M\n",
    "    ϕ(x) = 0.5*x'*K*x\n",
    "    δₓ = FiniteDiff.finite_difference_gradient(ϕ, x)\n",
    "    f = -K*δₓ\n",
    "    return f\n",
    "end\n",
    "\n",
    "function righthand_attractor_potential(x, ẋ, prob::Problem)\n",
    "    sys = prob.sys\n",
    "    M = 1.6\n",
    "    K = 2*M\n",
    "    ϕ(x) = 0.5*x'*K*x\n",
    "    δₓ = FiniteDiff.finite_difference_gradient(ϕ, x)\n",
    "    f = -K*δₓ\n",
    "    return f\n",
    "end\n",
    "\n",
    "function head_attractor_potential(x, ẋ, prob::Problem)\n",
    "    sys = prob.sys\n",
    "    M = 1.6\n",
    "    K = 3*M\n",
    "    ϕ(x) = 0.5*x'*K*x\n",
    "    δₓ = FiniteDiff.finite_difference_gradient(ϕ, x)\n",
    "    f = -K*δₓ\n",
    "    return f\n",
    "end\n",
    "\n",
    "function waist_attractor_potential(x, ẋ, prob::Problem)\n",
    "    sys = prob.sys\n",
    "    M = 1.6\n",
    "    K = 3*M\n",
    "    ϕ(x) = 0.5*x'*K*x\n",
    "    δₓ = FiniteDiff.finite_difference_gradient(ϕ, x)\n",
    "    f = -K*δₓ\n",
    "    return f\n",
    "end\n",
    "\n",
    "function dodge_potential(x, ẋ, prob::Problem)\n",
    "    sys = prob.sys \n",
    "    M = 1.6\n",
    "    K = M \n",
    "    s = [v > sys.max_range ? 0.0 : 1.0 for v in x]  \n",
    "    ϕ(σ) = (K/2) .* s .* (sys.max_range .- σ)./(sys.max_range .* σ).^2\n",
    "    δₓ = FiniteDiff.finite_difference_jacobian(ϕ, x) \n",
    "    f =-K*diag(δₓ)  \n",
    "    return f\n",
    "end\n",
    "\n",
    "\n",
    "### Solve \n",
    "function potential_eval(x, ẋ, name::Symbol, prob::Problem)\n",
    "    ϕ = eval(Symbol(name, :_potential))\n",
    "    f = ϕ(x, ẋ, prob)\n",
    "    return f\n",
    "end\n",
    " \n",
    "\n",
    "function dance(solve_potential, keypoints; horizon=3000)\n",
    "    init_joint_positions = [π/2, 2π/3, π/6, 2π/3, π/6, π/2, \n",
    "                            π/3, 7π/12, 2π/3, 5π/12]\n",
    "    sys = PickleRick(init_joint_positions,\n",
    "                    zero(init_joint_positions),\n",
    "                    nothing, keypoints)\n",
    "    sys.Δt = 5e-3\n",
    "    @show sys.Δt\n",
    "    tasks = [:waist_attractor, :lefthand_attractor, :righthand_attractor]\n",
    "    prob = Problem(tasks, sys)\n",
    "    θ = init_joint_positions\n",
    "    θ̇ = zero(init_joint_positions)\n",
    "    id=0\n",
    "    anim = @animate for i=1:horizon  \n",
    "        for (j,kp) in enumerate(sys.keypoints)\n",
    "            if i%kp.frequency == 0 \n",
    "                ind = (id%length(sys.keypoints))+1\n",
    "                sys.kpindex = ind \n",
    "                id +=1\n",
    "            end\n",
    "        end\n",
    "        τ = solve_potential(θ, θ̇ , prob, sys)\n",
    "        step_plots!(τ, prob, sys)\n",
    "        θ = prob.sys.θ; θ̇ = prob.sys.θ̇\n",
    "        visualize_plots!(sys) \n",
    "    end\n",
    "    return anim\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9875eb58",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44b84ec",
   "metadata": {},
   "source": [
    "The Glories of the Jacobian\n",
    "---------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b6c0c7",
   "metadata": {},
   "source": [
    "What exactly do we use Jacobians for? We can use Jacobians to make robots dance!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfde8a88",
   "metadata": {},
   "source": [
    "![](https://i.gifer.com/origin/7e/7ebb93d3dbda6e135782626385b2e10c_w200.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f70fcd",
   "metadata": {},
   "source": [
    "Math Dances Moves Video: <https://youtube.com/shorts/AHSlvTfsmCk?feature=shared>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177f0467",
   "metadata": {},
   "source": [
    "Before we start talking about dancing robots, let's first understand a few things about Jacobians.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0683062",
   "metadata": {},
   "source": [
    "From our discussions of differentiation approaches so far, we know that Jacobians are essentially matrices of partial derivatives of a function. For a function:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3f75dc",
   "metadata": {},
   "source": [
    "f(x\\_1, x\\_2, \\dots, x\\_n) = [y\\_1, y\\_2, \\dots, y\\_m]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575f6418",
   "metadata": {},
   "source": [
    "the Jacobian J can be expressed as:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027b4091",
   "metadata": {},
   "source": [
    "J = \\begin{bmatrix} \\frac{\\partial y\\_1}{\\partial x\\_1} & \\frac{\\partial y\\_1}{\\partial x\\_2} & \\dots & \\frac{\\partial y\\_1}{\\partial x\\_n} \\\\ \\frac{\\partial y\\_2}{\\partial x\\_1} & \\frac{\\partial y\\_2}{\\partial x\\_2} & \\dots & \\frac{\\partial y\\_2}{\\partial x\\_n} \\\\ \\dots & \\dots & \\dots & \\dots \\\\ \\frac{\\partial y\\_m}{\\partial x\\_1} & \\frac{\\partial y\\_m}{\\partial x\\_2} & \\dots & \\frac{\\partial y\\_m}{\\partial x\\_n} \\\\ \\end{bmatrix}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a283359",
   "metadata": {},
   "source": [
    "Staring at the Jacobian expression above, you'd notice that number of columns of the Jacobian matrix is equal to the dimension of the input vector of function f and the number of rows of the Jacobian matrix is equal to the dimension of the output vector. The Jacobian, in essence, tells us how a small change in the inputs of the function, affects the outputs of the function. The Jacobian is a linear map from the change in inputs of the function to the change in outputs of the function. And even cooler is the fact that, the pseudo-inverse of the Jacobian maps a small change in the outputs of a function to a small change in the inputs of the function. Let's look at some math that succinctly illustrates these ideas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebba266",
   "metadata": {},
   "source": [
    "\\Delta y = J \\cdot \\Delta x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1118806",
   "metadata": {},
   "source": [
    "\\Delta x = J^\\dagger \\Delta y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e3b4f6",
   "metadata": {},
   "source": [
    "J^\\dagger = J^\\top \\cdot (JJ^\\top)^{-1}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20faf46",
   "metadata": {},
   "source": [
    "where y is the vector of outputs of function f, x is the vector of inputs of function f, J is the Jacobian of function f and J^\\dagger is the right Moore-Penrose pseudo-inverse of J. [For future reference, J^\\dagger = J^\\top \\cdot (JJ^\\top)^{-1} is also simply called a pseudo right inverse. We used it for solving underdetermined equations earlier in the term.]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cf47a2",
   "metadata": {},
   "source": [
    "Having inherited the power of Jacobians, let us now use it for motion control.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ca05b3",
   "metadata": {},
   "source": [
    "An important function in robot motion control is the task map. A task map is a smooth, differentiable function that maps from the robot's configuration space to a pre-defined task space for a particular task. The robot's configuration space is the space of joint positions of the robot whilst the task space could be the space of the robot's end effector cartesian coordinates if the task is a hand or foot placement task, the space of center of mass positions if the task is a center of mass placement task, or even the space of joint configurations again if the task is a robot posture task. Tasks, task spaces and task maps are up to the robot operator to design.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c84bc09",
   "metadata": {},
   "source": [
    "Once these task maps are designed, the Jacobian of the task map will now allow us to map desired motions in the task space (which are intuitive to specify) to the required motions in the robot's configuration space (which are usually unintuitive to specify). This is the power of Jacobians!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ae71a3",
   "metadata": {},
   "source": [
    "To see this in action, let's write some code!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29364433",
   "metadata": {},
   "outputs": [],
   "source": [
    "kp1 = Keypoint(nothing, [-1.36, 3.2], [1.36, 3.2], [0.0,3.5], 40)\n",
    "kp2 = Keypoint(nothing, [1.36, 3.2], [-1.36, 3.2], [0.0, 2.5], 20) \n",
    "\n",
    "keypoints = [kp1, kp2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c905f7",
   "metadata": {},
   "source": [
    "We describe our dance motions using keypoints. Keypoints are the desired 2D cartesian positions. These positions are target positions to which we would like to move the robot's end effectors. The motions generated from moving the robot's end effectors to these keypoints are what will describe our dance motions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48bc5f3",
   "metadata": {},
   "source": [
    "The cell below shows the keypoints we describe for a particular dance routine.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fe98ac",
   "metadata": {},
   "source": [
    "The signature for describing a Keypoint is\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297db001",
   "metadata": {},
   "source": [
    "`Keypoint = (head_pose, lefthand_pose, righthand_pose, waist_pose, period)`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf07133",
   "metadata": {},
   "source": [
    "The `period` is an integer that describes how often the keypoint is repeated in the dance routine.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688a1416",
   "metadata": {},
   "source": [
    "#### **Here's where the magic happens.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b65e56e",
   "metadata": {},
   "source": [
    "In order to drive the robot's end effectors to the keypoints, we will need to apply a force to the robot's end effectors. How do we compute this force? We will use one of the coolest ideas in modern robotics called Artificial Potential Fields.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfa1fe5",
   "metadata": {},
   "source": [
    "Artificial potential fields are inspired by Potential Energy from Physics literature. From AP Physics, we know that Potential Energy is the kind of energy possessed by a body by virtue of its position relative to the other objects. For instance, a ball held at some height above the ground has a gravity potential energy proportional to its vertical position relative to the ground (i.e. its height). The force of gravity that pulls the object towards the ground can be computed by taking the negative derivative of the ball's gravitational potential energy with respect to its height.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5c6d01",
   "metadata": {},
   "source": [
    "Artificial potential fields behave in a similar manner. If we could build an artificial potential energy between a point of interest on a robot and a target position in space, we could compute the force needed to reach the target position by taking the negative derivative of this potential field. This idea provides as with an elegant and convenient approach for generating driving forces. To see how this is implemented in Julia, check out the accompanying `helper.jl` file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59721c06",
   "metadata": {},
   "source": [
    "Once we have generated the end-effector force, how do we figure out the right torques to apply to the joints of the robot to realize this force? Note that, the only way we are able to control our robot is through applying torques to its joints.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab86512",
   "metadata": {},
   "source": [
    "This is where the glory of the Jacobian shines!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4ee04b",
   "metadata": {},
   "source": [
    "We know that Jacobians allow us to map small changes from one space to small changes to another space. Since our force is in the end-effector space, or more formally, the task space, we can use our Jacobian to map this force to the configuration space, which is where the joints of the robot reside. All we have to do is to describe a task map that relates the robot's end effector to its target keypoint, compute the Jacobian of this task map, compute the force required to drive the end effector to the target keypoint and map that force to the robot's joint torques using the computed Jacobian. And voila, our robot will begin to dance!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff8ecdf",
   "metadata": {},
   "source": [
    "For brevity sake, we describe all the task maps and potential fields in the accompanying `helper.jl` file. Check it out for more details.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34419790",
   "metadata": {},
   "source": [
    "The function, `potential_solve`, below describes our routine for computing joint torque \\tau .\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104bdb16",
   "metadata": {},
   "source": [
    "In a for loop,\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3be24f1",
   "metadata": {},
   "source": [
    "\\* We loop through all the task maps for all the end-effectors of the robot (lefthand, righthand, waist).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8e4796",
   "metadata": {},
   "source": [
    "\\* We compute the task space coordinate x which in this case is the difference between the current end effector position and its corresponding target keypoint.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9401781c",
   "metadata": {},
   "source": [
    "\\* Next, we compute the Jacobian of the task map using Forward-mode Automatic Differentiation (we know all about this now)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac88798c",
   "metadata": {},
   "source": [
    "\\* We then compute the end-effector force needed to reach the target waypoint using artificial potential fields.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b491d14",
   "metadata": {},
   "source": [
    "\\* Having computed the Jacobians and forces for all the task maps, we compute the final joint torques by summing up the contributions of joint torques from the individual task maps computed using \\tau = J^\\top\\cdot f where J^\\top is the Jacobian transpose which approximates the Jacobian pseudo-inverse J^\\dagger and is faster to compute.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e2e5bf",
   "metadata": {},
   "source": [
    "For future reference, J^\\dagger = J^\\top \\cdot (JJ^\\top)^{-1} is called a pseudo right inverse. We used it for solving underdetermined equations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cc1fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "function potential_solve(θ, θ̇, prob::Problem, sys::PickleRick)\n",
    "    Js = []; fs = []\n",
    "    for t in prob.tasks \n",
    "        ψ = eval(Symbol(t, :_task_map))\n",
    "        x = ψ(θ, prob)\n",
    "        J = ForwardDiff.jacobian(σ->ψ(σ, prob), θ)\n",
    "        ẋ = J*θ̇\n",
    "        f = potential_eval(x, ẋ, t, prob)\n",
    "        push!(Js, J); push!(fs, f)\n",
    "    end\n",
    "    τ = vec(sum([J'*f for (J, f) in zip(Js, fs)]))  \n",
    "    return τ\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9651e6",
   "metadata": {},
   "source": [
    "Having implemented the `potential_solve` function, we will now pass it into our `dance` function alongside the dancing keypoints we described earlier. The `dance` function computes dance motions for 30 seconds (`horizon` = time\\_in\\_seconds \\* 100) and generates a video of the robot applying those motions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f768deda",
   "metadata": {},
   "source": [
    "The motion and video generation takes a few minutes so please be patient with it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00cae52",
   "metadata": {},
   "outputs": [],
   "source": [
    "anim = dance(potential_solve, keypoints; horizon=3000)\n",
    "gif(anim, \"dance.mp4\", fps=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d255e440",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "authors": [
   "Greg Werner"
  ],
  "kernelspec": {
   "display_name": "Julia 1.10.5",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
