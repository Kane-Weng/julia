{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94aa9829",
   "metadata": {},
   "source": [
    "## **Optimization**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6c9db5",
   "metadata": {},
   "source": [
    ">### Gradient Descent\n",
    "- **Goal**: Optimize the Rosenbrock function (classic test function used in optimization problems)  \n",
    "$$(1 - x)^2 + 100 * (y - x^2)^2$$\n",
    "- **Key Takeaways**\n",
    "    - create model: `Model(Ipopt.Optimizer) => register(model, :R, 2, R; autodiff = true)`\n",
    "    - define variables: `@variable(model, -20 <= x <= 20, start = -20)`\n",
    "    - define objective function: `@NLobjective(model, Min, R(x,y))`\n",
    "    - (solver options: `set_optimizer_attribute(...)`)\n",
    "    - solve optimization: `optimize!(model)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84edb86",
   "metadata": {},
   "source": [
    "Packages used:\n",
    "<span style=\"color:yellow; background-color:green\">JuMP, Ipopt</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e5e7ad",
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "using JuMP, Ipopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e49f2b96",
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal parameters (x, y): (0.9999999837814544, 0.9999999677255611)\n",
      "Minimum objective value: 2.656867929696428e-16\n"
     ]
    }
   ],
   "source": [
    "R(x,y) = (1-x)^2 + 100 * (y-x^2)^2\n",
    "\n",
    "# Create a new model with Ipopt as the solver\n",
    "model = Model(Ipopt.Optimizer)\n",
    "\n",
    "# Manually register the function with the model to use automatic differentiation\n",
    "register(model, :R, 2, R; autodiff = true)\n",
    "\n",
    "# Define variables & initial guess\n",
    "@variable(model, -20 <= x <= 20, start = -20)\n",
    "@variable(model, -20 <= y <= 20, start = 20)\n",
    "\n",
    "# No initial guess\n",
    "# @variable(model, -20 <= x <= 20)\n",
    "# @variable(model, -20 <= y <= 20)\n",
    "\n",
    "# Find the wrong minimum\n",
    "# @variable(model, -20 <= x <= 20, start = -1.2)\n",
    "# @variable(model, -20 <= y <= 20, start = 1)\n",
    "\n",
    "\n",
    "# Define objective function: Rosenbrock function\n",
    "@NLobjective(model, Min, R(x,y))\n",
    "\n",
    "# Solver options\n",
    "set_optimizer_attribute(model, \"max_iter\", 1000)    # Increase max iterations\n",
    "set_optimizer_attribute(model, \"tol\", 1e-9)         # tighter convergence tolerance\n",
    "set_optimizer_attribute(model, \"print_level\", 0)    # suppress all of the output => set to 0 (4 is compromised)\n",
    "\n",
    "# Solve the optimization problem\n",
    "optimize!(model)\n",
    "\n",
    "# Display results\n",
    "println(\"Optimal parameters (x, y): (\", value(x), \", \", value(y), \")\")\n",
    "println(\"Minimum objective value: \", objective_value(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1070ee3f",
   "metadata": {},
   "source": [
    ">### Gradient Descent - Multivariable example\n",
    "- **Goal**: Optimize Himmelblau's function (classic test function used in optimization problems)  \n",
    "$$f(x, y) = (x^2 + y - 11)^2 + (x + y^2 - 7)^2$$\n",
    "- **Key Takeaways**\n",
    "    - solve optimization using previous method\n",
    "    - compute hessian: `FiniteDiff.finite_difference_hessian(himmelblau_wrapper, [xStar, yStar])`\n",
    "    - compute eigenvalues: `eigvals(hessian)`\n",
    "    - Determine critical point's nature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4585d166",
   "metadata": {},
   "source": [
    "Packages used:\n",
    "<span style=\"color:yellow; background-color:green\">JuMP, Ipopt, FiniteDiff, LinearAlgebra</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e255676b",
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "using JuMP, Ipopt, FiniteDiff, LinearAlgebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43a2057a",
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "optimize_and_analyze_himmelblau (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Himmelblau function definition remains the same\n",
    "function himmelblau(x, y)\n",
    "    return (x^2 + y - 11)^2 + (x + y^2 - 7)^2\n",
    "end\n",
    "\n",
    "# Extended function to also compute Hessian and its eigenvalues\n",
    "function optimize_and_analyze_himmelblau(x_init, y_init)\n",
    "    model = Model(Ipopt.Optimizer)\n",
    "    \n",
    "    @variable(model, x, start = x_init)\n",
    "    @variable(model, y, start = y_init)\n",
    "    \n",
    "    @NLobjective(model, Min, (x^2 + y - 11)^2 + (x + y^2 - 7)^2)\n",
    "    \n",
    "    set_optimizer_attribute(model, \"max_iter\", 1000)\n",
    "    set_optimizer_attribute(model, \"tol\", 1e-9)\n",
    "    set_optimizer_attribute(model, \"print_level\", 0)\n",
    "    \n",
    "    optimize!(model)\n",
    "    \n",
    "    xStar, yStar = value(x), value(y)\n",
    "    println(\"======================================\")\n",
    "    println(\"Optimal parameters (x, y): (\", xStar, \", \", yStar, \")\")\n",
    "    println(\"Minimum objective value: \", objective_value(model))\n",
    "    \n",
    "    # Define a wrapper function for FiniteDiff\n",
    "    function himmelblau_wrapper(v)\n",
    "        return himmelblau(v[1], v[2])\n",
    "    end\n",
    "\n",
    "    # Compute hessian and eigenvalues\n",
    "    hessian = FiniteDiff.finite_difference_hessian(himmelblau_wrapper, [xStar, yStar])\n",
    "    eigenvalues = eigvals(hessian)\n",
    "\n",
    "    println(\"Hessian at the optimal point: \\n\", hessian)\n",
    "    println(\"Eigenvalues of the Hessian: \", eigenvalues)\n",
    "    \n",
    "    # Determine the nature of the critical point based on the eigenvalues\n",
    "    if all(eig -> eig > 0, eigenvalues)\n",
    "        println(\"The point is a local minimum.\")\n",
    "    elseif all(eig -> eig < 0, eigenvalues)\n",
    "        println(\"The point is a local maximum.\")\n",
    "    else\n",
    "        println(\"The point is a saddle point or the test is inconclusive.\")\n",
    "    end\n",
    "    println(\"======================================\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83b5c8fc",
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from point: (3, 2)\n",
      "======================================\n",
      "Optimal parameters (x, y): (3.0, 2.0)\n",
      "Minimum objective value: 0.0\n",
      "Hessian at the optimal point: \n",
      "[74.0000002682209 20.0; 20.0 34.00000011920929]\n",
      "Eigenvalues of the Hessian: [25.715728893569633, 82.28427149386056]\n",
      "The point is a local minimum.\n",
      "======================================\n",
      "Starting from point: (-2.805118, 3.283186)\n",
      "======================================\n",
      "Optimal parameters (x, y): (-2.805118086952745, 3.131312518250573)\n",
      "Minimum objective value: 7.888609052210118e-31\n",
      "Hessian at the optimal point: \n",
      "[64.949500088456 1.3047777251870691; 1.3047777251870691 80.44094498775873]\n",
      "Eigenvalues of the Hessian: [64.84037300554613, 80.5500720706686]\n",
      "The point is a local minimum.\n",
      "======================================\n",
      "Starting from point: (-3.77931, -3.283186)\n",
      "======================================\n",
      "Optimal parameters (x, y): (-3.779310253377774, -3.2831859912861767)\n",
      "Minimum objective value: 3.996369345849646e-26\n",
      "Hessian at the optimal point: \n",
      "[116.26548835580718 -28.24998497866954; -28.24998497866954 88.23448234835634]\n",
      "Eigenvalues of the Hessian: [70.71435509450659, 133.78561560965693]\n",
      "The point is a local minimum.\n",
      "======================================\n",
      "Starting from point: (3.584428, -1.848126)\n",
      "======================================\n",
      "Optimal parameters (x, y): (3.584428340330554, -1.8481265269646447)\n",
      "Minimum objective value: 9.516628633969033e-25\n",
      "Hessian at the optimal point: \n",
      "[104.78501259863707 6.945207253476179; 6.945207253476179 29.32457337913531]\n",
      "Eigenvalues of the Hessian: [28.690677260855026, 105.41890871691734]\n",
      "The point is a local minimum.\n",
      "======================================\n"
     ]
    }
   ],
   "source": [
    "# Starting points near the four local minima\n",
    "starting_points = [(3, 2), (-2.805118, 3.283186), (-3.779310, -3.283186), (3.584428, -1.848126)]\n",
    "\n",
    "# Optimize Himmelblau's function and analyze each result from the starting points\n",
    "for (x_init, y_init) in starting_points\n",
    "    println(\"Starting from point: (\", x_init, \", \", y_init, \")\")\n",
    "    optimize_and_analyze_himmelblau(x_init, y_init)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68b9624",
   "metadata": {},
   "source": [
    ">### Constrained Optimization - Lagrange's Method\n",
    "- **Goal**: Optimize the cost function under two constraint equations\n",
    "$$\n",
    "Cost(x,y,z) = -(x+y+z) \\\\\n",
    "g_1 = x^2 + y^2 -1 \\\\\n",
    "g_2 = x+z-2\n",
    "$$   \n",
    "\n",
    "- **Key Takeaways**\n",
    "    - symbolic systems (defining)\n",
    "    - Lagrangian function: $L(x,y,z) = Cost(x,y,z) + \\lambda_1 * g_1(x,y,z) + \\lambda_2 * g_2(x,y,z)$\n",
    "    - compute gradient: `Symbolics.gradient(L, [var;lam])`\n",
    "    - solve for gradL = 0: `nlsolve(gradL_num, initial_guess)`\n",
    "    - process the solutions to find minimum\n",
    "- **JuMP & Ipopt application**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610a36f5",
   "metadata": {},
   "source": [
    "Packages used:\n",
    "<span style=\"color:yellow; background-color:green\">Symbolics, NLsolve, Random</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6f47c77",
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "using Symbolics, NLsolve, Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10f42ecc",
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$ \\begin{equation}\n",
       "\\left[\n",
       "\\begin{array}{c}\n",
       "-1 + \\mathtt{lam}_{2} + 2 \\mathtt{lam}_{1} \\mathtt{var}_{1} \\\\\n",
       "-1 + 2 \\mathtt{lam}_{1} \\mathtt{var}_{2} \\\\\n",
       "-1 + \\mathtt{lam}_{2} \\\\\n",
       "-1 + \\left( \\mathtt{var}_{1} \\right)^{2} + \\left( \\mathtt{var}_{2} \\right)^{2} \\\\\n",
       "-2 + \\mathtt{var}_{1} + \\mathtt{var}_{3} \\\\\n",
       "\\end{array}\n",
       "\\right]\n",
       "\\end{equation}\n",
       " $$"
      ],
      "text/plain": [
       "5-element Vector{Num}:\n",
       " -1 + lam[2] + 2lam[1]*var[1]\n",
       "          -1 + 2lam[1]*var[2]\n",
       "                  -1 + lam[2]\n",
       "     -1 + var[1]^2 + var[2]^2\n",
       "         -2 + var[1] + var[3]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#=====================SYMBOLIC SOLUTION APPROACH=======================#\n",
    "\n",
    "# Define symbolic variables\n",
    "@Symbolics.variables var[1:3]\n",
    "@Symbolics.variables lam[1:2]\n",
    "\n",
    "# Define cost function\n",
    "Cost = -(var[1] + var[2] + var[3])\n",
    "\n",
    "# Define constraint equations\n",
    "g1 = var[1]^2 + var[2]^2 - 1\n",
    "g2 = var[1] + var[3] - 2\n",
    "\n",
    "# Formulate the Lagrangian: objective function + lagrange multipliers for constraints\n",
    "L = Cost + lam[1]*g1 + lam[2]*g2\n",
    "\n",
    "# Compute the gradient\n",
    "gradL = Symbolics.gradient(L, [var;lam])\n",
    "\n",
    "display(gradL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "42d32af2",
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[var_val; lam_val] = [1.8163521787042063e-11, -1.0000000000098037, 1.9999999999818365, -0.49999999998886063, 1.0]\n",
      "[var_val; lam_val] = [1.5028135762532895e-14, 1.0000000000000162, 1.999999999999985, 0.4999999999999901, 1.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2-element Vector{Float64}:\n",
       " -3.000000000000016\n",
       " -0.9999999999901963"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#=====================NUMERICAL SOLUTION APPROACH=======================#\n",
    "\n",
    "# Convert symbolic expression to numerical function\n",
    "gradL_num = build_function(gradL, [var; lam])\n",
    "gradL_num = eval(gradL_num[1])\n",
    "\n",
    "# Initialize an array to store solutions\n",
    "cost_solutions = Float64[]\n",
    "\n",
    "# Solve with multiple initial guesses\n",
    "for k = 1:100\n",
    "    initial_guess = randn(5)\n",
    "    result = nlsolve(gradL_num, initial_guess, xtol=1e-6, ftol=1e-7)\n",
    "\n",
    "    # Process converged solutions\n",
    "    if converged(result)\n",
    "        var_val = result.zero[1:3]\n",
    "        lam_val = result.zero[4:5]\n",
    "        cost_extremePoint = -sum(var_val)\n",
    "\n",
    "        # Add the found solution if sufficiently diff from previous found solution\n",
    "        if all(abs(cost_extremePoint-sol) > 1e-2 for sol in cost_solutions)\n",
    "            push!(cost_solutions, cost_extremePoint)\n",
    "            @show [var_val; lam_val]\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "sorted_solutions = sort(cost_solutions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3cd73ba1",
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective value: -3.0\n",
      "Solution: \n",
      "var[1] = 4.232217986118638e-17\n",
      "var[2] = 1.0\n",
      "var[3] = 2.0\n"
     ]
    }
   ],
   "source": [
    "#=====================SOLVE USING JuMP=======================#\n",
    "model = Model(Ipopt.Optimizer)\n",
    "@variable(model, var[1:3])\n",
    "@NLobjective(model, Min, -(var[1]+var[2]+var[3]))\n",
    "\n",
    "# Add constraint equations\n",
    "@NLconstraint(model, var[1]^2 + var[2]^2 == 1)\n",
    "@NLconstraint(model, var[1] + var[3] == 2)\n",
    "\n",
    "set_optimizer_attribute(model, \"max_iter\", 1000)\n",
    "set_optimizer_attribute(model, \"tol\", 1e-9)\n",
    "set_optimizer_attribute(model, \"print_level\", 0)\n",
    "    \n",
    "optimize!(model)\n",
    "\n",
    "# Display the results\n",
    "println(\"Objective value: \", objective_value(model))\n",
    "println(\"Solution: \")\n",
    "println(\"var[1] = \", value(var[1]))\n",
    "println(\"var[2] = \", value(var[2]))\n",
    "println(\"var[3] = \", value(var[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df2a151",
   "metadata": {},
   "source": [
    ">### Constrained gradient descent\n",
    "- **Goal**: Optimize the cost function under equality constraints\n",
    "$$\n",
    "Cost(x_1,x_2,x_3,x_4) = x_1^2 + x_2^2 + x_3^2 + x_4^2\\\\\n",
    "G_1(x_1,x_2,x_3,x_4) = x_1 + x_2 - 3\\\\ \n",
    "G_2(x_1,x_2,x_3,x_4) = sin(\\pi x_1) - x_3^2 - 1 + x_4^3\n",
    "$$   \n",
    "\n",
    "- **Requirements**\n",
    "    - Decreasing direction: $\\nabla f(x_k) * \\Delta x < 0$\n",
    "    - Following constraints: $\\nabla g_i(x_k) * \\Delta x = 0$\n",
    "    - Formulas used (remove components of $\\nabla g_i(x_k)$ from $\\nabla f(x_k)$):\n",
    "$$\n",
    "\\mathrm{proj}_{\\mathbf{b}} \\mathbf{a} = \\left( \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{\\mathbf{b} \\cdot \\mathbf{b}} \\right) \\mathbf{b}\\\\\n",
    "\\Delta x = - \\nabla f(x_k) + \\sum_{i=1}^{m} a_i\\nabla g_i(x_k)\\\n",
    "$$\n",
    "- **Key Takeaways**\n",
    "    - symbolic systems (defining)\n",
    "    - Lagrangian function: $L(x,y,z) = Cost(x,y,z) + \\lambda_1 * g_1(x,y,z) + \\lambda_2 * g_2(x,y,z)$\n",
    "    - compute gradient: `Symbolics.gradient(L, [var;lam])`\n",
    "    - solve for gradL = 0: `nlsolve(gradL_num, initial_guess)`\n",
    "    - process the solutions to find minimum\n",
    "- **JuMP & Ipopt application**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b1c3e0",
   "metadata": {},
   "source": [
    "Packages used:\n",
    "<span style=\"color:yellow; background-color:green\">ForwardDiff, LinearAlgebra, NLsolve</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8289c87b",
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "using ForwardDiff, LinearAlgebra, NLsolve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "322724ee",
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JacG (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the cost function f(x) and the equality constraints G(x)\n",
    "function cost(x)\n",
    "    return x[1]^2 + x[2]^2 + x[3]^2 + x[4]^2\n",
    "end\n",
    " \n",
    "function G(x)\n",
    "    return [x[1] + x[2] - 3, sin(pi*x[1]) - x[3]^2 - 1 + x[4]^3]\n",
    "end\n",
    " \n",
    "# Compute the gradient of f and the Jacobian of G using ForwardDiff\n",
    "grad_cost(x) = ForwardDiff.gradient(cost, x)\n",
    "JacG(x) = ForwardDiff.jacobian(G, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "186c75ad",
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gram_schmidt (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#=================================== FIND THE ORTHONORMAL BASIS OF JACOBIAN_G =====================================#\n",
    "function gram_schmidt(jacobian_G)\n",
    "    n, m = size(jacobian_G)\n",
    "    B = zeros(n,m)  # Basis orthonormal Vectors to be returned\n",
    "\n",
    "    for j=1:m\n",
    "        # start with original vectors\n",
    "        b = jacobian_G[:, j]    \n",
    "        \n",
    "        for i = 1:j-1\n",
    "            b -= dot(B[:,i], jacobian_G[:, j]) * B[:,i]\n",
    "        end\n",
    "        B[:,j] = b / norm(b)\n",
    "    end\n",
    "    \n",
    "    return B\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e5252b22",
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "project_grad_cost (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#======================== PROJECT THE GRADIENT ONTO THE FEASIBLE DIRECTIONS =================================#\n",
    "function project_grad_cost(grad_cost, B)\n",
    "    # Iterate over columns of B, subtracting their projection from grad_f\n",
    "    projected_grad_cost = grad_cost\n",
    "    for j = 1:size(B, 2)\n",
    "        projected_grad_cost = projected_grad_cost - dot(grad_cost, B[:,j]) * B[:,j]     # vectors in B are already orthonormal, so no need to divide by norm(B[:,j])\n",
    "    end\n",
    "    return projected_grad_cost\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0a2d0869",
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "find_feasible_start (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Find feasible starting point using NLsolve\n",
    "function find_feasible_start(G, x_guess)\n",
    "    function constraint!(F, x)\n",
    "        F[1:2] = G(x)\n",
    "    end\n",
    "    xFeasible = nlsolve(constraint!, x_guess)\n",
    "    return xFeasible.zero\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2588106c",
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Iteration 184 ----\n",
      "Met tolerances.\n",
      "JacCost applied to null(JacG) (should be near 0): [0.0 -0.0]\n",
      "Constraints (should be near 0): [-0.0, 2.0e-5]\n",
      "----------------- \n",
      "\n",
      "Final Results:\n",
      "-----------------\n",
      "Optimal x: [0.95451, 2.04549, 0.0, 0.95008]\n",
      "Optimal cost(xStar): 5.99778\n"
     ]
    }
   ],
   "source": [
    "# Actual algorithm\n",
    "x_guess = [0.0, 0.0, 0.0, 0.0] # Not feasible\n",
    "x0 = find_feasible_start(G, x_guess) # Make feasible\n",
    "  \n",
    "# set step size, maxIter, tolerances\n",
    "s=0.1\n",
    "maxIter=10000\n",
    "gradTol = 1e-6\n",
    "constraintTol = 1e-4\n",
    " \n",
    "xk = x0\n",
    "\n",
    "for k=1:maxIter\n",
    "    jacobian_G_transposed = transpose(JacG(xk))\n",
    "    B = gram_schmidt(jacobian_G_transposed)\n",
    "    projected_grad_cost = project_grad_cost(grad_cost(xk), B)\n",
    "    if norm(projected_grad_cost) < gradTol\n",
    "        println(\"---- Iteration $k ----\")\n",
    "        println(\"Met tolerances.\")\n",
    "        println(\"JacCost applied to null(JacG) (should be near 0): \", \n",
    "           round.((grad_cost(xk)')*nullspace(JacG(xk)), digits=5))\n",
    "        println(\"Constraints (should be near 0): \", round.(G(xk), digits=5))\n",
    "        println(\"----------------- \")\n",
    "        break\n",
    "    else\n",
    "        # Update\n",
    "        xk = xk - s*projected_grad_cost\n",
    "    end\n",
    "    if norm(G(xk)) > constraintTol\n",
    "        xk = find_feasible_start(G, xk) # Again Feasible\n",
    "    end\n",
    "end\n",
    "xStar = xk\n",
    "costStar = cost(xStar)\n",
    "\n",
    "println(\"\\nFinal Results:\")\n",
    "println(\"-----------------\")\n",
    "println(\"Optimal x: \", round.(xStar, digits=5))\n",
    "\n",
    "println(\"Optimal cost(xStar): \", round(costStar, digits=5))\n",
    " "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
